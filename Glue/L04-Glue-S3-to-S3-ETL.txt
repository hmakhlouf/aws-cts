 ========================================================
   Lab 4: ETL from one S3 bucket to another S3 bucket
 ========================================================

   1. Load the required data to S3
	1.1 Create two folders in s3 to read from and write to:
		s3://iquiz.glue/movies-s3-to-s3/read
		s3://iquiz.glue/movies-s3-to-s3/write
	1.2 Load the dataset to the read folder
		Upload the movies-data.csv file to 's3://iquiz.glue/movies-s3-to-s3/read' folder

   2. Create an IAM Role (CTSGlueRole) for Glue with the following policies:			
	Service: Glue
	Policies:   
	  -> AmazonS3FullAccess
	  -> AWSGlueServiceRole

   3. Connect to AWS Glue service and create a Glue crawler:
	3.1  In the left panel of the Glue management console click Crawlers.
	3.2  Click the blue Add crawler button.
	3.3  Give the crawler a name such as glue-s3-s3-crawler.
	3.4  In Add a data store menu choose S3 and select the bucket you created. 
		-> Drill down to select the folder : s3://iquiz.glue/movies-s3-to-s3/read
	3.5  In Choose an IAM role 
	    -> Create an IAM role
		-> IAM role: AWSGlueServiceRole-S3toS3
	3.6  In Configure the crawler’s output add a database called glue-s3-s3-db.

   4. Run the crawler
	4.1  Goto Crawlers link to list all crawlers
	4.2  Select the crawler and Click 'Run crawler' button.

		Notes:  
		------
		Once the data has been crawled, the crawler creates a metadata table from it. 
		You find the results from the Tables section of the Glue console. 
		The database that you created during the crawler setup is just an arbitrary way 
		of grouping the tables.

		Glue tables don’t contain the data but only a schema to access the data.

   5. Create the Glue job
	5.1  Name the job as 'glue-s3-s3-job'.
	5.2  Choose the same IAM role (CTSGlueRole) that you created for the crawler. 
	5.3  Type: Spark.
	5.4  Glue version: Spark 2.4, Python 3 (or whatever is the default)
	 	-> This job runs: A new script to be authored by you.
	5.5  Security configuration, script libraries, and job parameters
		-> Worker type - Standard
	 	-> Number of workers: 2.  (This is the minimum and costs about 0.15$ per run)
	 	-> Job timeout: 10. Prevents the job to run longer than expected.
	5.6  Click Next and then 'Save job and edit the script'.


   6. Provide the script for your ETL functionality
	6.1 Write the script in the editor (glue-s3-to-s3.py)
	6.2 Save the code in the editor - Click save button.
        6.3 click Run job.

   7. Check the results
	7.1 Once the the job is complete the results of the script are stored in output s3 folder
	    -> Output s3 folder: s3://iquiz.glue/movies-s3-to-s3/write/


----------------------------------------------
    pyspark script (glue-s3-to-s3.py)
----------------------------------------------


#########################################
### IMPORT LIBRARIES AND SET VARIABLES
#########################################

#Import python modules
from datetime import datetime

#Import pyspark modules
from pyspark.context import SparkContext
import pyspark.sql.functions as f

#Import glue modules
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job

#Initialize contexts and session
spark_context = SparkContext.getOrCreate()
glue_context = GlueContext(spark_context)
session = glue_context.spark_session

#Parameters
glue_db = "glue-s3-s3-db"
glue_tbl = "read"
s3_write_path = "s3://iquiz.glue/movies-s3-to-s3/write/"

#########################################
### EXTRACT (READ DATA)
#########################################

#Log starting time
dt_start = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
print("Start time:", dt_start)

#Read movie data to Glue dynamic frame
dynamic_frame_read = glue_context.create_dynamic_frame.from_catalog(database = glue_db, table_name = glue_tbl)

#Convert dynamic frame to data frame to use standard pyspark functions
data_frame = dynamic_frame_read.toDF()

#########################################
### TRANSFORM (MODIFY DATA)
#########################################

#Create a decade column from year
decade_col = f.floor(data_frame["year"]/10)*10
data_frame = data_frame.withColumn("decade", decade_col)

#Group by decade: Count movies, get average rating
data_frame_aggregated = data_frame.groupby("decade").agg(
    f.count(f.col("movie_title")).alias('movie_count'),
    f.mean(f.col("rating")).alias('rating_mean'),
)

#Sort by the number of movies per the decade
data_frame_aggregated = data_frame_aggregated.orderBy(f.desc("movie_count"))

#Print result table
#Note: Show function is an action. Actions force the execution of the data frame plan.
#With big data the slowdown would be significant without cacching.
data_frame_aggregated.show(10)

#########################################
### LOAD (WRITE DATA)
#########################################

#Create just 1 partition, because there is so little data
data_frame_aggregated = data_frame_aggregated.repartition(1)

#Convert back to dynamic frame
dynamic_frame_write = DynamicFrame.fromDF(data_frame_aggregated, glue_context, "dynamic_frame_write")

#Write data back to S3
glue_context.write_dynamic_frame.from_options(
    frame = dynamic_frame_write,
    connection_type = "s3",
    connection_options = {
        "path": s3_write_path,
        #Here you could create S3 prefixes according to a values in specified columns
        #"partitionKeys": ["decade"]
    },
    format = "csv"
)

#Log end time
dt_end = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
print("End time:", dt_end)
